{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shw10-DanilBokatenko",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVAygknQX7jt"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "This notebook is largely based on the course on [Practical RL](https://github.com/yandexdataschool/Practical_RL): [reference link](https://github.com/yandexdataschool/Practical_RL/blob/master/week06_policy_based/reinforce_pytorch.ipynb)\n",
        "\n",
        "Here we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qy7Ybf420kV"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aosokin/dl_cshse_ami/blob/master/2021-fall/homeworks_small/shw10/DL21-fall-shw10.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNX2IP2UX7jt",
        "outputId": "075f46f0-39f8-43fe-f086-ed6d0f7808f6"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIdPBHWWX7jt"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eOABeuhX7jt"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "lJYyqd12X7ju",
        "outputId": "a9568cca-cbdf-4ad8-c515-12c73577f946"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f955d35ef90>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ZrYvoZX7ju"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALC6Gt-MX7ju"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u72fskDGX7ju"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap6y7QHWX7ju"
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  # <YOUR CODE: define a neural network that predicts policy logits>\n",
        "  nn.Linear(state_dim[0], state_dim[0]),\n",
        "  # nn.BatchNorm1d(state_dim[0]),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(state_dim[0], n_actions),\n",
        "  # nn.BatchNorm1d(n_actions),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(n_actions, n_actions),\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mUBMieAX7ju"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5gJjrc9X7ju"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e57rgiXvX7ju"
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    with torch.no_grad():\n",
        "      x = torch.tensor(states, dtype=torch.float)\n",
        "      # print(x.shape)\n",
        "      x = model(x)\n",
        "      x = F.softmax(x, dim=-1).numpy()\n",
        "    return x"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtUxAB9AX7ju"
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu6bRGizX7ju"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji3FqW3WX7ju"
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(np.arange(n_actions), p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j68WQSOhX7ju"
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyzxApMhX7ju"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APWwH0yVX7ju"
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    cumsum = [rewards[-1]]\n",
        "    g = gamma\n",
        "    for r in rewards[:-1][::-1]:\n",
        "      cumsum.append(cumsum[-1] * gamma + r)\n",
        "    return cumsum[::-1]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-lLJb3UX7ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2915d0-ee80-4798-dc90-3c64eac130f6"
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2uFzW6TX7ju"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNFGvKp3X7ju"
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABaU0L5lX7jv"
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    entropy = torch.sum(log_probs_for_actions * cumulative_returns, dim=-1)\n",
        "    loss = -entropy.mean() * entropy_coef\n",
        "\n",
        "    # Gradient descent step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc-w5BfSX7jv"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flcq3TCsX7jv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154d4c12-7997-4166-81f1-b1aa949c6439"
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 500:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:14.400\n",
            "mean reward:16.490\n",
            "mean reward:19.650\n",
            "mean reward:18.510\n",
            "mean reward:21.760\n",
            "mean reward:23.320\n",
            "mean reward:24.050\n",
            "mean reward:24.060\n",
            "mean reward:25.990\n",
            "mean reward:26.830\n",
            "mean reward:28.540\n",
            "mean reward:30.270\n",
            "mean reward:33.250\n",
            "mean reward:36.610\n",
            "mean reward:38.020\n",
            "mean reward:39.660\n",
            "mean reward:42.340\n",
            "mean reward:49.810\n",
            "mean reward:52.400\n",
            "mean reward:52.560\n",
            "mean reward:58.170\n",
            "mean reward:61.200\n",
            "mean reward:68.400\n",
            "mean reward:107.020\n",
            "mean reward:123.040\n",
            "mean reward:170.430\n",
            "mean reward:238.590\n",
            "mean reward:334.640\n",
            "mean reward:399.550\n",
            "mean reward:528.330\n",
            "You Win!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J-B06FfX7jv"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyqIG1fPX7jv"
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUD7am78X7jv",
        "colab": {
          "resources": {
            "http://localhost:8080/videos/openaigym.video.0.60.video000064.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "bae78179-e716-4aaa-a5d9-acb428dcf645"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"videos/openaigym.video.0.60.video000064.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIasJYwtMA8s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}