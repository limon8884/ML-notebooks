{"nbformat":4,"nbformat_minor":4,"metadata":{"notebookPath":"homework_machine_translation_de-en/default.ipynb","language_info":{"pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"notebookId":"b4c944ee-7dc0-48bd-bf2f-2b0ee09c148e","kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"ydsNotebookPath":"homework_machine_translation_de-en/1.ipynb"},"cells":[{"cell_type":"code","source":"# качаем данные\nimport requests #код взят от семинариста, разрешение на использование получено)\n\ndef download_file_from_google_drive(id, destination):\n    URL = \"https://docs.google.com/uc?export=download\"\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { 'id' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { 'id' : id, 'confirm' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, \"wb\") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n                \ndownload_file_from_google_drive('15S0KQwoGPJAZ1mclzVz8OHOp1FChSwpf', 'file.zip')","metadata":{"cellId":"54958586-6c17-4bcf-8de1-ec95586937be"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import zipfile # вроде стандартная либа\nwith zipfile.ZipFile('file.zip', 'r') as zip_ref:\n    zip_ref.extractall()\n    ","metadata":{"cellId":"b4a604c0-dddb-44dc-a322-94d31c4ff874"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#!g1.1\nfrom IPython.display import clear_output\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\n# import sacrebleu \nimport torchtext\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Transformer\nimport math\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDEVICE","metadata":{"cellId":"793582bb-cf81-4a86-98eb-335e2c4d9dc9","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"#!g1.1\n\ndef yield_sentences(filename, n=None):\n    with open(filename, encoding='utf-8', mode='r') as f:\n        line = f.readline().split()\n        i = 0\n        while line and (n is None or i < n):\n            yield line\n            line = f.readline().split()\n            i += 1\n            ","metadata":{"cellId":"1406d019-cae6-4a05-afec-513339953ad6","trusted":true},"outputs":[],"execution_count":89},{"cell_type":"code","source":"#!g1.1\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n\ntrain_path_de = \"train.de-en.de\"\ntrain_path_en = \"train.de-en.en\"\n\nval_path_de = \"val.de-en.de\"\nval_path_en = \"val.de-en.en\"\n\ntrain_small_path_de = \"train_small.de-en.de\"\ntrain_small_path_en = \"train_small.de-en.en\"\n\nvocab_de = build_vocab_from_iterator(yield_sentences(train_path_de), specials=special_symbols, special_first=True)\nvocab_en = build_vocab_from_iterator(yield_sentences(train_path_en), specials=special_symbols, special_first=True)\nvocab_en.set_default_index(UNK_IDX)\nvocab_de.set_default_index(UNK_IDX)","metadata":{"cellId":"b1251fda-d7f3-48be-899a-3ed75b6bd2b9","trusted":true},"outputs":[],"execution_count":90},{"cell_type":"code","source":"#!g1.1\n# код далее взят из торчовского туториала про использование трансформера\n\n# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\nclass PositionalEncoding(nn.Module):\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\n# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\n# Seq2Seq Network\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int = 512,\n                 dropout: float = 0.1, \n                 activation='relu'):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = Transformer(d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout, \n                                       activation=activation)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                src_mask: Tensor,\n                tgt_mask: Tensor,\n                src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor,\n                memory_key_padding_mask: Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)","metadata":{"cellId":"1b612768-97b7-4c04-94bb-60815b919abb","trusted":true},"outputs":[],"execution_count":91},{"cell_type":"code","source":"#!g1.1\ndef generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask","metadata":{"cellId":"0d7ca737-5151-4193-86a6-cec44a22f77b","trusted":true},"outputs":[],"execution_count":92},{"cell_type":"code","source":"#!g1.1\ntorch.manual_seed(0)\nSRC_VOCAB_SIZE = len(vocab_de)\nTGT_VOCAB_SIZE = len(vocab_en)","metadata":{"cellId":"ip0xzdvvqpme3046v6ejvu","trusted":true},"outputs":[],"execution_count":93},{"cell_type":"code","source":"#!g1.1\n\nEMB_SIZE = 512 * 2\nNHEAD = 8\nFFN_HID_DIM = 512\nBATCH_SIZE = 128\nNUM_ENCODER_LAYERS = 4\nNUM_DECODER_LAYERS = 4\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","metadata":{"cellId":"18f63772-287b-4c03-a975-de8c0017653d","trusted":true},"outputs":[],"execution_count":94},{"cell_type":"code","source":"#!g1.1\nfrom torch.nn.utils.rnn import pad_sequence\nfrom typing import Iterable, List\n\n# function to add BOS/EOS and create tensor for input sequence indices\ndef tensor_transform(token_ids: List[int]):\n    return torch.cat((torch.tensor([BOS_IDX]),\n                      torch.tensor(token_ids),\n                      torch.tensor([EOS_IDX])))\n\n# function to collate data samples into batch tesors\ndef collate_fn(batch):\n    src_batch, tgt_batch = [], []\n    for src_sample, tgt_sample in batch:\n        src_batch.append(tensor_transform(vocab_de(src_sample)))\n        tgt_batch.append(tensor_transform(vocab_en(tgt_sample)))\n\n    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n    return src_batch, tgt_batch","metadata":{"cellId":"8b17e8b6-e0d0-4bfc-9be8-5c698405925e","trusted":true},"outputs":[],"execution_count":95},{"cell_type":"code","source":"#!g1.1\nfrom torch.utils.data import IterableDataset\n\nclass MyIterableDataset(IterableDataset):\n    def __init__(self, path_de, path_en) -> None:\n        super().__init__()\n        self.path_de = path_de\n        self.path_en = path_en\n\n    def yielder(self):\n        for a in zip(yield_sentences(self.path_de), yield_sentences(self.path_en)):\n            yield a\n\n    def __len__(self):\n        with open(self.path_de, encoding='utf-8', mode='r') as f:\n            return len(f.readlines())\n\n    def __iter__(self):\n        return self.yielder()\n    \n","metadata":{"cellId":"2a1e746d-6af8-4551-a5d0-8661366783b9","trusted":true},"outputs":[],"execution_count":96},{"cell_type":"code","source":"#!g1.1\nclass SmoothingCrossEntropyLoss(nn.Module):\n    def __init__(self, label_smoothing=0.0, ignore_index=-100):\n        super(SmoothingCrossEntropyLoss, self).__init__()\n        assert 0 <= label_smoothing <= 0.5\n        self.smoothing = label_smoothing\n        self.ignore_index = ignore_index\n\n    def forward(self, y_pred, y_true):\n        pred = y_pred.log_softmax(dim=-1)\n        \n        with torch.no_grad():\n            ind = (y_true == self.ignore_index)\n            not_ignored_num = len(y_true) - torch.sum(ind).item()\n            w = torch.full(pred.shape, self.smoothing / (TGT_VOCAB_SIZE - 1)).to(DEVICE)\n            w.scatter_(1, y_true.data.unsqueeze(1), 1 - self.smoothing)\n            w[ind, :] = 0.0\n        return torch.mean(torch.sum(-w * pred, dim=1)) * len(y_true) / not_ignored_num ","metadata":{"cellId":"xguomd8q5lhqam3yl94un","trusted":true},"outputs":[],"execution_count":97},{"cell_type":"code","source":"#!g1.1\nfrom torch.utils.data import DataLoader\n\ndef train_epoch(model, optimizer, scheduler=None, swa_transformer=None, return_loader=False, update_grad_freq=1, train_small=False, smoothing=None):\n    model.train()\n    losses = 0\n    if train_small:\n        train_iter = MyIterableDataset(train_small_path_de, train_small_path_en)\n    else:\n        train_iter = MyIterableDataset(train_path_de, train_path_en)\n    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n    if smoothing is not None:\n        loss_fn = SmoothingCrossEntropyLoss(smoothing, ignore_index=PAD_IDX)\n\n    optimizer.zero_grad()\n    for batch_num, (src, tgt) in enumerate(tqdm(train_dataloader)):\n        \n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:-1, :]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        loss.backward()\n        \n        if (batch_num + 1) % update_grad_freq == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            if scheduler is not None:\n                scheduler.step()\n        losses += loss.item()\n        \n        \n        \n        \n#     if scheduler is not None and swa_transformer is None:\n#         scheduler.step(loss.item())\n        \n    \n    if return_loader:\n        return losses / len(train_dataloader), train_dataloader\n    return losses / len(train_dataloader)\n\n\ndef evaluate(model):\n    model.eval()\n    losses = 0\n\n    # val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    val_dataloader = DataLoader(MyIterableDataset(val_path_de, val_path_en), batch_size=BATCH_SIZE, collate_fn=collate_fn)\n    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n    \n    for src, tgt in val_dataloader:\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:-1, :]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        losses += loss.item()\n\n    return losses / len(val_dataloader)","metadata":{"cellId":"ccc2645f-8043-4019-bdb3-8f12d3a2929e","trusted":true},"outputs":[],"execution_count":98},{"cell_type":"code","source":"#!g1.1\n# FINAL education\n\nfrom torch.optim.lr_scheduler import StepLR, ExponentialLR, ReduceLROnPlateau, CyclicLR, OneCycleLR\nfrom timeit import default_timer as timer\nNUM_EPOCHS = 15\nmax_lr = 0.0001\nxbs=4\nloader_len = 1531\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM, activation='gelu')\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\ntransformer = transformer.to(DEVICE)\n\noptimizer = torch.optim.Adam(transformer.parameters(), lr=0.0, betas=(0.9, 0.98), eps=1e-9)\nscheduler = OneCycleLR(optimizer, max_lr=max_lr, epochs=NUM_EPOCHS, steps_per_epoch=loader_len // xbs + 1, pct_start=0.15, anneal_strategy='cos')\nscheduler_pl = ReduceLROnPlateau(optimizer, patience=3, verbose=True, factor=0.5)\n\nSOTA_val_results = []\nlrs = []\n\nsched_epoch = 5\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    lrs.append(scheduler.get_last_lr()) \n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer, scheduler=scheduler, update_grad_freq=xbs, train_small=False, smoothing=0.1) \n    end_time = timer()\n    val_loss = evaluate(transformer)\n    if epoch > sched_epoch:\n        scheduler_pl.step(val_loss)\n    SOTA_val_results.append(val_loss)\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n","metadata":{"cellId":"gifxbw04or8jgxb1y5xku","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edb97fea05324daf81b62fb9ce301cdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d38a3f350844068182e0b2d68052cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16a5dfb91af7447aad4966325dddd11b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c923535de74d08b05bc31cc2040582"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6467ca07124c20a2de25cc23d201b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d9acd7a22ae47ecba053775bd909133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23ee8ad147b344f0bc0efe948e251b62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"812d4386ffe14349a21be491d7485647"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c0982b972ce42e994e1f3f35c29b5fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7631ae715484a38900534beb8bbea68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67c3507856a48ab829b0842f769cd9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aef40d3e71f4881a14e4259727d9d33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc1d2681f85747d0850ca2c7f5cb2e6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24fc5b7fddd8428f89acba590157323f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\nEpoch: 1, Train loss: 8.642, Val loss: 6.087, Epoch time = 1978.743s\n\nEpoch: 2, Train loss: 6.017, Val loss: 4.549, Epoch time = 1982.179s\n\nEpoch: 3, Train loss: 5.000, Val loss: 3.654, Epoch time = 1986.753s\n\nEpoch: 4, Train loss: 4.408, Val loss: 3.089, Epoch time = 1984.015s\n\nEpoch: 5, Train loss: 4.016, Val loss: 2.833, Epoch time = 1981.933s\n\nEpoch: 6, Train loss: 3.744, Val loss: 2.673, Epoch time = 1984.623s\n\nEpoch: 7, Train loss: 3.548, Val loss: 2.593, Epoch time = 1984.424s\n\nEpoch: 8, Train loss: 3.397, Val loss: 2.530, Epoch time = 1980.055s\n\nEpoch: 9, Train loss: 3.279, Val loss: 2.493, Epoch time = 1977.020s\n\nEpoch: 10, Train loss: 3.187, Val loss: 2.478, Epoch time = 1981.791s\n\nEpoch: 11, Train loss: 3.118, Val loss: 2.440, Epoch time = 1987.672s\n\nEpoch: 12, Train loss: 3.066, Val loss: 2.445, Epoch time = 1993.397s\n\nEpoch: 13, Train loss: 3.030, Val loss: 2.447, Epoch time = 1983.114s\n\nEpoch: 14, Train loss: 3.009, Val loss: 2.447, Epoch time = 1979.868s\n"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07e9e116d93145fb93ca0c23f4878342"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\nEpoch: 15, Train loss: 3.000, Val loss: 2.449, Epoch time = 1981.118s\n"}],"execution_count":99},{"cell_type":"code","source":"# #!g1.1\n# # дообучение (решил не делать, не верю, что даст что-то)\n\n# from torch.optim.lr_scheduler import StepLR, ExponentialLR, ReduceLROnPlateau, CyclicLR, OneCycleLR\n# from timeit import default_timer as timer\n\n# # # load model\n# # transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM, activation='gelu')\n# # checkpoint = torch.load('model_final.pt')\n\n# # transformer.load_state_dict(checkpoint['model_state_dict'])\n# # transformer = transformer.to(DEVICE)\n\n# NUM_EPOCHS = 3\n# max_lr = 0.0001\n# xbs=8\n# loader_len = 1531\n\n# optimizer = torch.optim.SGD(transformer.parameters(), lr=0.0, momentum=0.9)\n# scheduler = OneCycleLR(optimizer, max_lr=max_lr, epochs=NUM_EPOCHS, steps_per_epoch=loader_len // xbs + 1, pct_start=0.5, anneal_strategy='linear')\n# scheduler_pl = ReduceLROnPlateau(optimizer, patience=3, verbose=True, factor=0.5)\n\n# SOTA_val_results_2 = []\n# lrs = []\n\n# sched_epoch = 0\n\n# for epoch in range(1, NUM_EPOCHS+1):\n#     lrs.append(scheduler.get_last_lr()) \n#     start_time = timer()\n#     train_loss = train_epoch(transformer, optimizer, scheduler=scheduler, update_grad_freq=xbs, train_small=False, smoothing=0.01) \n#     end_time = timer()\n#     val_loss = evaluate(transformer)\n#     if epoch > sched_epoch:\n#         scheduler_pl.step(val_loss)\n#     SOTA_val_results_2.append(val_loss)\n#     print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n","metadata":{"cellId":"0r9x9b9wtw4nkaifdtkyni","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1531.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd9e9b0fd7574b7493d176026f788475"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\n"},{"output_type":"error","ename":"NotImplementedError","evalue":"","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)","\u001B[0;32m<ipython-input-3-b94bc197b770>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0mlrs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscheduler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_last_lr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m     \u001B[0mstart_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtimer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m     \u001B[0mtrain_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_epoch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransformer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscheduler\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mscheduler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mupdate_grad_freq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mxbs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_small\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msmoothing\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m     \u001B[0mend_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtimer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[0mval_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransformer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m<ipython-input-11-e64e699f9ea5>\u001B[0m in \u001B[0;36mtrain_epoch\u001B[0;34m(model, optimizer, scheduler, swa_transformer, return_loader, update_grad_freq, train_small, smoothing)\u001B[0m\n","\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    232\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__iter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    233\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 234\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtqdm_notebook\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__iter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    235\u001B[0m                 \u001B[0;31m# return super(tqdm...) will not catch exception\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    236\u001B[0m                 \u001B[0;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1173\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1174\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0miterable\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1175\u001B[0m                 \u001B[0;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1176\u001B[0m                 \u001B[0;31m# Update and possibly print the progressbar.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    519\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    520\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 521\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    522\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    523\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    559\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    560\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 561\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    562\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    563\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mT_co\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mNotImplementedError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__add__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mother\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'Dataset[T_co]'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m'ConcatDataset[T_co]'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mNotImplementedError\u001B[0m: "]}],"execution_count":107},{"cell_type":"code","source":"#!g1.1\n# function to generate output sequence using greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n\n        ys = torch.cat([ys,\n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\n\ndef translate(model: torch.nn.Module, src_sentence: str):\n    model.eval()\n    src = tensor_transform(vocab_de(src_sentence.split())).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n    return \" \".join(vocab_en.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\ntranslate(transformer, 'ich hasse kinder !')","metadata":{"cellId":"6c79779d-b838-4fcc-8cbc-0a430db5f77e","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"' i hate kids ! '"},"metadata":{}}],"execution_count":101},{"cell_type":"code","source":"#!g1.1\nmy_file_path_de = \"test1.de-en.de\"\nwith open(my_file_path_de, encoding='utf-8', mode='r') as f_opened, open('test1.de-en.en', encoding='utf-8', mode='w') as f_answer:\n    line = f_opened.readline()\n    while line:\n        f_answer.write(translate(transformer, line).strip() + '\\n')\n        line = f_opened.readline()","metadata":{"cellId":"o31a7tma08bv98cq3hp1e","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/kernel/lib/python3.7/site-packages/ml_kernel/kernel.py:859: UserWarning: The following variables cannot be serialized: f_answer, f_opened\n  warnings.warn(message)\n"}],"execution_count":102},{"cell_type":"code","source":"#!g1.1\n# заархиватор\nfrom zipfile import ZipFile\nzipped_file = ZipFile('answer1.zip', 'w')\nzipped_file.write('test1.de-en.en')\nzipped_file.close()","metadata":{"cellId":"gkdrxrtdmh2ovxnvnap0i","trusted":true},"outputs":[],"execution_count":103},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"f659c80b-8d45-4ae4-8fb0-eab05765b749"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"cnpsl93xq3m6n520ilchen"},"outputs":[],"execution_count":null}]}