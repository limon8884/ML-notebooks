{"nbformat":4,"nbformat_minor":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"notebookId":"2474acb5-dfca-49bd-b852-98ae7cc3263a","kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"ydsNotebookPath":"DL-HW1-BokatenkoDanil.ipynb"},"cells":[{"cell_type":"code","source":"","metadata":{"cellId":"spdp9ldfdg9e8veuo3laow"},"outputs":[],"execution_count":257},{"cell_type":"code","source":"# качаем данные\nimport requests #код взят от семинариста, разрешение на использование получено)\n\ndef download_file_from_google_drive(id, destination):\n    URL = \"https://docs.google.com/uc?export=download\"\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { 'id' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { 'id' : id, 'confirm' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, \"wb\") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)","metadata":{"cellId":"n5tx1eqotq8s4toou17pa"},"outputs":[],"execution_count":81},{"cell_type":"code","source":"download_file_from_google_drive('1WkbQQuFL5dCaPGQz_YRu5Ne6-OEgj8Qs', 'file.zip')","metadata":{"cellId":"5k53itw88qw5jxokkkoi1"},"outputs":[],"execution_count":71},{"cell_type":"code","source":"import zipfile # вроде стандартная либа\nwith zipfile.ZipFile('file.zip', 'r') as zip_ref:\n    zip_ref.extractall()\n    ","metadata":{"cellId":"i21p3ac7dehduq040zxvth"},"outputs":[],"execution_count":74},{"cell_type":"code","source":"#!g1.1\n# ставим, че просили\n%pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n%pip install pandas==1.3.3","metadata":{"cellId":"3ti28xvajvhlfdknkwrnfg"},"outputs":[],"execution_count":356},{"cell_type":"code","source":"#!g1.1\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\n# import torchvision.transforms as transforms\nfrom torchvision import transforms\nfrom IPython.display import clear_output\nfrom tqdm.auto import tqdm\nimport pandas as pd\nfrom PIL import Image\n# import matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.optim.swa_utils import AveragedModel, SWALR\nfrom random import randint\nimport random\n# %matplotlib inline  \n","metadata":{"cellId":"oiv4h8oinqp70a8nqbob96"},"outputs":[],"execution_count":1009},{"cell_type":"code","source":"#!g1.1 \n# выгружаем данные на куду\ndef ind_to_path(a):\n    s = str(a)\n    return 'simple_image_classification/trainval/trainval_' + '0' * (5 - len(s)) + s + '.jpg'\n\nclass TrainDataset(torch.utils.data.Dataset):\n    def __init__(self, from_=0, to_=90000):\n        trans = transforms.ToTensor()\n        norm = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        X_list = []\n        for i in range(from_, to_):\n            image = ind_to_path(i)\n            im=Image.open(image)\n            tens = trans(im)\n#             tens = norm(tens)\n            X_list.append(tens)\n        labels = pd.read_csv('simple_image_classification/labels_trainval.csv')\n        self.y = torch.tensor(np.array(labels.Category[from_:to_])).cuda()\n#         self.X = torch.stack(X_list).cuda()\n        self.X = X_list\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]","metadata":{"cellId":"pt9d92fbetlifrkaqtksp"},"outputs":[],"execution_count":1182},{"cell_type":"code","source":"#!g1.1 MODEL_1\n# DIFFERENT BLOCKS - RESNET18, RESNET50, GOOGLENET\n\nclass Debug(nn.Module):\n    def forward(self, x):\n        print(x.shape)\n        return x\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size()[0], -1)\n\nclass BasicBlock18(nn.Module):\n    def __init__(self, chans, block_depth=2, twice_decrease=False, use_dropout=True, params={'eps': 1e-05, 'momentum': 0.1, 'bias': False, 'p': 0.2}):\n        super().__init__()\n        self.twice_decrease = twice_decrease\n        self.block_depth = block_depth\n        self.use_dropout = use_dropout\n        k = 1\n        if twice_decrease:\n            k = 2\n        layers = []\n        for i in range(self.block_depth):\n            k_start = k\n            if i == 0:\n                k_start = 1\n            layers.append(nn.Conv2d(chans*k_start, chans*k, kernel_size=(3, 3), stride=(k - k_start + 1, k - k_start + 1), padding=(1, 1), bias=params['bias']))\n            layers.append(nn.BatchNorm2d(chans*k, eps=params['eps'], momentum=params['momentum'], affine=True, track_running_stats=True))\n            layers.append(nn.ReLU(inplace=True))\n            if use_dropout:\n                layers.append(nn.Dropout(params['p']))\n        self.layers = nn.Sequential(*layers)\n        self.l6_ = nn.Conv2d(chans, chans*2, kernel_size=(1, 1), stride=(2, 2), bias=params['bias'])\n        self.l7_ = nn.BatchNorm2d(chans*2, eps=params['eps'], momentum=params['momentum'], affine=True, track_running_stats=True)\n    def forward(self, input):\n        x = input\n        x = self.layers(x)\n        if self.twice_decrease:\n            x_ = input\n            x_ = self.l6_(x_)\n            x_ = self.l7_(x_)\n            return x + x_\n        return x + input\n\nclass BasicBlock50(nn.Module):\n    def __init__(self, chans, twice_decrease=False, use_dropout=False):\n        super().__init__()\n        self.twice_decrease = twice_decrease\n        self.use_dropout = use_dropout\n        k = 1\n        if twice_decrease:\n            k = 2\n        \n        layers = []\n        layers.append(nn.Conv2d(chans, chans*k//4, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0), bias=False)),\n        if use_dropout:\n            layers.append(nn.Dropout(0.2))\n        layers.append(nn.BatchNorm2d(chans*k//4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n        layers.append(nn.ReLU(inplace=True)),\n        layers.append(nn.Conv2d(chans*k//4, chans*k//4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n        layers.append(nn.BatchNorm2d(chans*k//4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n        if use_dropout:\n            layers.append(nn.Dropout(0.2))\n        layers.append(nn.ReLU(inplace=True)),\n        layers.append(nn.Conv2d(chans*k//4, chans*k, kernel_size=(1, 1), stride=(k, k), padding=(k//2, k//2), bias=False)),\n        layers.append(nn.BatchNorm2d(chans*k, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n        if use_dropout:\n            layers.append(nn.Dropout(0.2))\n        \n        self.layers = nn.Sequential(*layers)\n        self.l6_ = nn.Conv2d(chans, chans*2, kernel_size=(1, 1), stride=(2, 2), padding=(1, 1), bias=False)\n        self.l7_ = nn.BatchNorm2d(chans*2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.activation = nn.ReLU(inplace=True)\n    def forward(self, input):\n        x = input\n        x = self.layers(x)\n        if self.twice_decrease:\n            x_ = input\n            x_ = self.l6_(x_)\n            x_ = self.l7_(x_)\n            return self.activation(x + x_)\n        return self.activation(x + input)\n\nclass GoogleBlock(nn.Module):\n    def __init__(self, chans, use_dropout=False):\n        super().__init__()\n        self.inc = chans\n        self.use_dropout = use_dropout \n        \n        self.l1 = nn.Conv2d(chans, chans//4, kernel_size=1, padding=0)\n        self.dr1 = nn.Dropout(0.2)\n        self.bn1 = nn.BatchNorm2d(chans//4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        \n        self.l2_1 = nn.Conv2d(chans, chans//2, kernel_size=1, padding=0)\n        self.dr2_1 = nn.Dropout(0.2)\n        self.l2_2 = nn.Conv2d(chans//2, chans//2, kernel_size=3, padding=1)\n        self.dr2_2 = nn.Dropout(0.2)\n        self.bn2 = nn.BatchNorm2d(chans//2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        \n        self.l3_1 = nn.Conv2d(chans, chans//16, kernel_size=1, padding=0)\n        self.dr3_1 = nn.Dropout(0.2)\n        self.l3_2 = nn.Conv2d(chans//16, chans//8, kernel_size=5, padding=2)\n        self.dr3_2 = nn.Dropout(0.2)\n        self.bn3 = nn.BatchNorm2d(chans//8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        self.l4_1 = nn.MaxPool2d(kernel_size=3, padding=1, stride=1)\n        self.dr4_1 = nn.Dropout(0.2)\n        self.l4_2 = nn.Conv2d(chans, chans//8, kernel_size=1, padding=0)\n        self.dr4_2 = nn.Dropout(0.2)\n        self.bn4 = nn.BatchNorm2d(chans//8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        \n    def forward(self, input):\n        out = []\n        x1 = input\n        x1 = F.relu(self.bn1(self.l1(x1)))\n        if self.use_dropout:\n            x1 = self.dr1(x1)       \n        out.append(x1)\n        x2 = input\n        x2 = F.relu(self.l2_1(x2))\n        if self.use_dropout:\n            x2 = self.dr2_1(x2)\n        x2 = F.relu(self.bn2(self.l2_2(x2)))\n        if self.use_dropout:\n            x2 = self.dr2_2(x2)\n        out.append(x2)\n        x3 = input\n        x3 = F.relu(self.l3_1(x3))\n        if self.use_dropout:\n            x3 = self.dr3_1(x3)\n        x3 = F.relu(self.bn3(self.l3_2(x3)))\n        if self.use_dropout:\n            x3 = self.dr3_2(x3)   \n        out.append(x3)\n        x4 = input\n        x4 = F.relu(self.l4_1(x4))\n        if self.use_dropout:\n            x4 = self.dr4_1(x4)\n        x4 = F.relu(self.bn4(self.l4_2(x4)))\n        if self.use_dropout:\n            x4 = self.dr4_2(x4)          \n        out.append(x4)\n        return torch.cat(out, dim=1)\n","metadata":{"cellId":"zlh47lf8koi56tztk7tw9"},"outputs":[],"execution_count":1102},{"cell_type":"code","source":"#!g1.1\n\n# BEST MODELS OF DIFFERENT TYPES\n\nclass BestModel(nn.Module):\n    def __init__(self, params={'eps': 1e-05, 'momentum': 0.1, 'bias': False, 'p': 0.3}):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=params['bias']), \n            nn.BatchNorm2d(80, eps=params['eps'], momentum=params['momentum'], affine=True, track_running_stats=True), \n            nn.ReLU(),\n            BasicBlock18(80, 3, params=params),\n            BasicBlock18(80, 2, params=params),\n            BasicBlock18(80, 1, params=params),\n            BasicBlock18(80, 4, params=params),\n            BasicBlock18(80, 2, twice_decrease=True, params=params),\n            BasicBlock18(160, 1, params=params),\n            BasicBlock18(160, 2, params=params),\n            BasicBlock18(160, 3, params=params),\n            BasicBlock18(160, 4, params=params),\n            BasicBlock18(160, twice_decrease=True, params=params),\n            BasicBlock18(320, 1, params=params),\n            BasicBlock18(320, 3, params=params),\n            BasicBlock18(320, 4, params=params),\n            BasicBlock18(320, 2, twice_decrease=True, params=params),\n            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n            Flatten(),\n            nn.Linear(640, 200)\n        )\n    def forward(self, input):\n        return self.model(input)\n\n\n# SOTA 0.38 - 5 epochs\n# TURN_ON_DROPOUT=True\n# model = nn.Sequential(\n#     nn.Conv2d(3, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), #\n#     nn.BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \n#     nn.ReLU(),\n#     BasicBlock18(80, 3, use_dropout=TURN_ON_DROPOUT),\n#     BasicBlock18(80, 2, use_dropout=TURN_ON_DROPOUT),\n#     BasicBlock18(80, 1, use_dropout=TURN_ON_DROPOUT),\n#     BasicBlock18(80, 2, twice_decrease=True, use_dropout=TURN_ON_DROPOUT),\n#     BasicBlock18(160, 1, use_dropout=TURN_ON_DROPOUT),\n#     BasicBlock18(160, 2, use_dropout=TURN_ON_DROPOUT),\n#     BasicBlock18(160, 3, use_dropout=TURN_ON_DROPOUT),\n#     BasicBlock18(160, twice_decrease=True, use_dropout=TURN_ON_DROPOUT),\n#     BasicBlock18(320, 1, use_dropout=TURN_ON_DROPOUT),\n#     BasicBlock18(320, 3, use_dropout=TURN_ON_DROPOUT),\n#     BasicBlock18(320, 2, twice_decrease=True, use_dropout=TURN_ON_DROPOUT),\n#     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#     Flatten(),\n#     nn.Linear(640, 200)\n# )\n\n# resnet50 0.23 - 5 epochs\n# model = nn.Sequential(\n#     nn.Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False), #\n#     nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \n#     nn.ReLU(inplace=True),\n#     BasicBlock50(32, twice_decrease=True),\n#     BasicBlock50(64, twice_decrease=True),\n#     BasicBlock50(128),\n#     BasicBlock50(128, twice_decrease=True),\n#     BasicBlock50(256),\n#     BasicBlock50(256, twice_decrease=True),\n#     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#     Flatten(),\n#     nn.Linear(512, 200)\n# )\n\n\n\n# BASELINE FROM CHECKPOINT resnet18: 0.27 - 3 epochs; 0.33 - 5 epochs\n# model = nn.Sequential(\n#     nn.Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False), #\n#     nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \n#     nn.ReLU(inplace=True),\n#     BasicBlock18(32, use_dropout=TORN_ON_DROPOUT),\n#     BasicBlock18(32, use_dropout=TORN_ON_DROPOUT),\n#     BasicBlock18(32, twice_decrease=True, use_dropout=TORN_ON_DROPOUT),\n#     BasicBlock18(64, use_dropout=TORN_ON_DROPOUT),\n#     BasicBlock18(64, twice_decrease=True, use_dropout=TORN_ON_DROPOUT),\n#     BasicBlock18(128, use_dropout=TORN_ON_DROPOUT),\n#     BasicBlock18(128, twice_decrease=True, use_dropout=TORN_ON_DROPOUT),\n#     BasicBlock18(256, use_dropout=TORN_ON_DROPOUT),\n#     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#     Flatten(),\n#     nn.Linear(256, 200)\n# )\n\n# Googlenet 0.27 - 5 epochs\n\n# model = nn.Sequential(\n#     nn.Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False), #\n#     nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \n#     nn.ReLU(),\n    \n#     GoogleBlock(32, use_dropout=TURN_ON_DROPOUT),\n#     nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n#     GoogleBlock(64, use_dropout=TURN_ON_DROPOUT),\n#     nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n#     GoogleBlock(128, use_dropout=TURN_ON_DROPOUT),\n#     nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n#     GoogleBlock(256, use_dropout=TURN_ON_DROPOUT),\n#     nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n#     GoogleBlock(512, use_dropout=TURN_ON_DROPOUT),\n    \n#     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#     Flatten(),\n#     nn.Linear(512, 200)\n# )\n\n","metadata":{"cellId":"z1m7yqxhx4ibggn17p2wsj"},"outputs":[],"execution_count":1420},{"cell_type":"code","source":"#!g1.1\n# EXPEREMENTING WITH DIFFERENT MODELS\n\n# 0.3\n# model = nn.Sequential(\n#     nn.Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False), #\n#     nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \n#     nn.ReLU(inplace=True),\n#     BasicBlock18(32, block_depth=4),\n# #     nn.LayerNorm((32, 40, 40)),\n#     BasicBlock18(32, block_depth=5),\n#     BasicBlock18(32),\n#     BasicBlock18(32, block_depth=3),\n# #     nn.LayerNorm((32, 40, 40)),\n#     BasicBlock18(32, block_depth=3, twice_decrease=True),\n#     BasicBlock18(64, block_depth=4),\n#     BasicBlock18(64),\n# #     nn.LayerNorm((64, 20, 20)),\n#     BasicBlock18(64, twice_decrease=True),\n#     BasicBlock18(128, block_depth=3),\n#     BasicBlock18(128, block_depth=4),\n# #     nn.LayerNorm((128, 10, 10)),\n#     BasicBlock18(128, twice_decrease=True),\n#     BasicBlock18(256, block_depth=3),\n#     BasicBlock18(256),\n# #     nn.LayerNorm((256, 5, 5)),\n#     BasicBlock18(256, twice_decrease=True),\n#     BasicBlock18(512, block_depth=3),\n# #     nn.LayerNorm((512, 3, 3)),\n#     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#     Flatten(),\n#     nn.Linear(512, 200)\n# )\n\n# SOTA at some moment\n# model = nn.Sequential(\n#     nn.Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False), #\n#     nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \n#     nn.ReLU(inplace=True),\n#     BasicBlock18(32),\n#     BasicBlock18(32),\n#     BasicBlock18(32, twice_decrease=True),\n#     BasicBlock18(64),\n#     BasicBlock18(64, twice_decrease=True),\n#     BasicBlock18(128),\n#     BasicBlock18(128, twice_decrease=True),\n#     BasicBlock18(256),\n#     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#     Flatten(),\n#     nn.Linear(512, 200)\n# )\n\n# Trying to make several branches with several outputs\n# branch_start = nn.Sequential(\n#     nn.Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False), #\n#     nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \n#     nn.ReLU(inplace=True),\n#     BasicBlock18(32),\n#     BasicBlock18(32),\n#     BasicBlock18(32, twice_decrease=True),\n#     BasicBlock18(64),\n#     BasicBlock18(64, twice_decrease=True),\n# )\n\n# branch1 = nn.Sequential(\n#     nn.Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False),\n#     nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n#     nn.ReLU(inplace=True),\n#     nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n#     nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n#     nn.ReLU(inplace=True),\n#     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#     Flatten(), #512\n#     nn.LayerNorm((512)),\n# )\n\n# branch2 = nn.Sequential(\n#     BasicBlock18(128),\n#     BasicBlock18(128, twice_decrease=True),\n#     BasicBlock18(256),\n#     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#     Flatten(),\n#     nn.LayerNorm((256)),\n# )\n\n# branch3 = nn.Sequential(\n#     BasicBlock18(128, 3),\n#     BasicBlock18(128, 3, twice_decrease=True),\n#     BasicBlock18(256, 3, twice_decrease=True),\n#     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#     Flatten(),\n#     nn.LayerNorm((512)),\n# )\n\n\n# class MyModel(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.bs = branch_start\n#         self.b1 = branch1\n#         self.b2 = branch2\n#         self.b3 = branch3\n#         self.ll = nn.Linear(512+512+256, 200)\n# #         self.b2 = branch2\n#     def forward(self, input):\n#         xs = self.bs(input)\n#         x1 = self.b1(xs)\n#         x2 = self.b2(xs)\n#         x3 = self.b3(xs)\n#         return self.ll(torch.cat((x1, x2, x3), 1))\n\n# model = MyModel()","metadata":{"cellId":"1rsgxh28qb6mduepbjb1mb"},"outputs":[],"execution_count":825},{"cell_type":"code","source":"#!g1.1\n# RANDOMLY DEFINED MODEL FOR SEARCHING THE BEST ARCHITECTURE\n\nclass FindBlock(nn.Module):\n    def __init__(self, chans, use_dropout=False, twice_decrease=False):\n        super().__init__()\n        self.block = None\n        choice_list = ['resnet18'] #['resnet18', 'resnet50', 'googleblock']\n        self.type_ = None\n        if twice_decrease:\n            self.type_ = 'resnet18'\n        else:\n            self.type_ = random.choice(choice_list)\n        if self.type_ == 'resnet18':\n            block_depth = random.randint(1, 4)\n            self.type_ += '-' + str(block_depth)\n            self.block = BasicBlock18(chans, block_depth=block_depth, use_dropout=use_dropout, twice_decrease=twice_decrease)\n#         elif self.type_ == 'resnet50':\n#             self.block = BasicBlock50(chans, use_dropout=use_dropout, twice_decrease=twice_decrease)\n        elif self.type_ == 'googleblock':\n            self.block = GoogleBlock(chans, use_dropout=use_dropout)\n    def forward(self, input):\n        return self.block(input)\n    \n\nclass FindModel(nn.Module):\n    def __init__(self, min_depth=4, max_depth=20, start_num_channes=32, num_downs=3, use_dropout=False):\n        super().__init__()\n        self.block_names = []\n        self.start_num_channes = start_num_channes\n        self.num_downs = num_downs\n        self.depth = random.randint(min_depth, max_depth)\n        downsample_indexes = set(random.sample(range(self.depth), num_downs))\n        \n        x = start_num_channes\n        layers = [\n            nn.Conv2d(3, x, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), #\n            nn.BatchNorm2d(x, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \n            nn.ReLU()\n        ]\n        \n        for i in range(self.depth):\n            twice_decrease = False\n            if i in downsample_indexes:\n                twice_decrease = True\n            block = FindBlock(x, use_dropout=use_dropout, twice_decrease=twice_decrease)\n            layers.append(block)\n            name = block.type_\n            if twice_decrease:\n                x *= 2\n                name += ' x2'\n            self.block_names.append(name)\n        \n        layers += [\n            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n            Flatten(),\n            nn.Linear(x, 200)\n        ]\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def represent(self):\n        return '\\n'.join(self.block_names)\n    \n    def forward(self, input):\n        return self.layers(input)\n    \n\n# model = FindModel()","metadata":{"cellId":"h0sphgdqyue2heegek0icm"},"outputs":[],"execution_count":1076},{"cell_type":"code","source":"#!g1.1\n# объявляем лоадеры\n\nbatch_size = 128\nborder = 99000\n\ntrain_loader = torch.utils.data.DataLoader(dataset=TrainDataset(from_=0, to_=border), \n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=TrainDataset(from_=border, to_=100000), \n                                          batch_size=batch_size, \n                                          shuffle=True)\n","metadata":{"cellId":"g871d4sjfeph3nhioe9u"},"outputs":[],"execution_count":1389},{"cell_type":"code","source":"#!g1.1\n# Функции обучения\nend_softmax = nn.LogSoftmax(dim=-1)\n\naugmentation_list = [\n                    transforms.ColorJitter(brightness=.5, hue=.3), transforms.RandomRotation(degrees=(0, 90)), transforms.RandomPosterize(2), \n                    transforms.RandomAdjustSharpness(2), transforms.RandomHorizontalFlip(), \n                    transforms.RandomPerspective(distortion_scale=0.6, p=1.0),\n                    transforms.RandomPerspective(distortion_scale=0.6, p=1.0), transforms.RandomRotation(degrees=(0, 60)), transforms.RandomRotation(degrees=(0, 180)),\n                    transforms.RandomCrop(size=40), transforms.CenterCrop(size=40), transforms.CenterCrop(size=40), transforms.RandomCrop(size=40),\n                    transforms.RandomCrop(size=40), transforms.RandomCrop(size=40), transforms.RandomCrop(size=40), transforms.RandomCrop(size=40), \n                    ]\n\n\ndef batch_to_tensor(batch):\n    norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    return torch.stack([norm(tens) for tens in batch]).cuda()\n\n\ndef apply_random_aug(batch):\n    augmented_list = []\n    for tens in batch:\n        augment_applier = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.RandomChoice(augmentation_list),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n        augmented_list.append(augment_applier(tens))        \n    return torch.stack(augmented_list).cuda()\n\n\ndef train_epoch(model, optimizer, scheduler=None, batchsize=batch_size, use_aug=False, use_mixup=False):\n    loss_log, acc_log = [], []\n    model.train()\n    saved_batch = None\n    saved_labels = None\n\n    for batch_num, (x_batch, y_batch) in enumerate(tqdm(train_loader)):\n#         data = x_batch\n        data = batch_to_tensor(x_batch)\n        if use_aug:\n            data = apply_random_aug(x_batch)\n        target = y_batch\n        optimizer.zero_grad()\n        \n        if use_mixup:\n            acc_log.append(0.0)\n            if batch_num % 2 == 0:\n                saved_batch = data\n                saved_labels = target\n            elif batch_num != border // batch_size:\n                l = np.random.rand()\n                new_data = data * l + saved_batch * (1 - l)\n                output = end_softmax(model(new_data))\n                loss = F.nll_loss(output, target) * l + F.nll_loss(output, saved_labels) * (1 - l)\n                loss.backward()\n                optimizer.step()\n                if scheduler:\n                    scheduler.step()\n                loss = loss.item()\n                loss_log.append(loss)\n        else:\n            output = end_softmax(model(data))\n            pred = torch.max(output, 1)[1]\n            acc = torch.eq(pred, y_batch).float().mean()\n            acc_log.append(acc)\n            loss = F.nll_loss(output, target)#.cpu()\n            loss.backward()\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            loss = loss.item()\n            loss_log.append(loss)\n            \n        \n                \n    return loss_log, acc_log    \n\ndef test(model, plato_scheduler=None):\n    loss_log, acc_log = [], []\n    model.eval()\n    for batch_num, (x_batch, y_batch) in enumerate(tqdm(test_loader)):    \n        data = batch_to_tensor(x_batch)\n#         data = x_batch     \n        target = y_batch\n\n#         output = model(data)\n        output = end_softmax(model(data))\n        loss = F.nll_loss(output, target)#.cpu()\n\n        pred = torch.max(output, 1)[1]\n        acc = torch.eq(pred, y_batch).float().mean()\n        acc_log.append(acc)\n        \n        loss = loss.item()\n        loss_log.append(loss)\n    if plato_scheduler:\n        plato_scheduler.step(torch.mean(torch.tensor(loss_log)))\n    return loss_log, acc_log\n\n\n","metadata":{"cellId":"l0s66evxo0fdkyqj5dncnn"},"outputs":[],"execution_count":1398},{"cell_type":"code","source":"#!g1.1\n# Итоговое обучение\n\nmodel = BestModel().cuda()\n\nlog_accs = []\nlog_tr_accs = []\nopt2 = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\nfor i in range(10):    \n    use_aug=True\n    print(i)\n    train_loss, train_acc = train_epoch(model, opt2, scheduler=None, batchsize=batch_size, use_aug=use_aug)\n    clear_output()\n    val_loss, val_acc = test(model, plato_scheduler=None)\n    acc = torch.mean(torch.tensor(val_acc)).item()\n    log_accs.append(acc)\n    log_tr_accs.append(torch.mean(torch.tensor(train_acc)).item())\n\n\nopt2 = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\ncycle_scheduler = lr_scheduler.OneCycleLR(opt2, max_lr=0.0001, total_steps=30*(border//batch_size+1))\nplato_scheduler = lr_scheduler.ReduceLROnPlateau(opt2, 'min', verbose=True, patience=2)\nfor i in range(30):\n    use_aug=True\n    print(10+i)\n    train_loss, train_acc = train_epoch(model, opt2, scheduler=cycle_scheduler, batchsize=batch_size, use_aug=use_aug)\n    clear_output()\n    val_loss, val_acc = test(model, plato_scheduler=plato_scheduler)\n    acc = torch.mean(torch.tensor(val_acc)).item()\n    log_accs.append(acc)\n    log_tr_accs.append(torch.mean(torch.tensor(train_acc)).item())\n    print(acc)\n    \nopt1 = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\nstep_scheduler = lr_scheduler.StepLR(opt1, step_size=border//batch_size*20, gamma=0.1)\nplato_scheduler = lr_scheduler.ReduceLROnPlateau(opt1, 'min', verbose=True, patience=10)\nfor i in range(80):\n    print(40+i)\n    use_mixup=False\n    train_loss, train_acc = train_epoch(model, opt1, scheduler=step_scheduler, batchsize=batch_size, use_aug=True, use_mixup=use_mixup)\n    clear_output()\n    val_loss, val_acc = test(model, plato_scheduler=plato_scheduler)\n    acc = torch.mean(torch.tensor(val_acc)).item()\n    log_accs.append(acc)\n    tr_acc = torch.mean(torch.tensor(train_acc)).item()\n    lr = step_scheduler.get_last_lr()[0]\n    print(acc )","metadata":{"cellId":"cug634rpm3a83pvymvvz85"},"outputs":[],"execution_count":1421},{"cell_type":"code","source":"# #!g1.1\n# # SEARCHING FOR OPTIMAL METHOD OF OPTIMIZATION\n\n# model = BestModel().cuda()\n\n# log_accs = []\n# log_tr_accs = []\n# opt2 = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n# for i in range(10):    \n#     use_aug=True\n#     print(i)\n#     train_loss, train_acc = train_epoch(model, opt2, scheduler=None, batchsize=32, use_aug=use_aug)\n#     clear_output()\n#     val_loss, val_acc = test(model, plato_scheduler=None)\n#     acc = torch.mean(torch.tensor(val_acc)).item()\n#     log_accs.append(acc)\n#     log_tr_accs.append(torch.mean(torch.tensor(train_acc[-(90000//32):])).item())\n# #     plt.plot(log_accs)\n# #     plt.plot(log_tr_accs)\n# #     plt.show()\n\n# opt2 = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n# cycle_scheduler = lr_scheduler.OneCycleLR(opt2, max_lr=0.0001, total_steps=20*90000//32)\n# plato_scheduler = lr_scheduler.ReduceLROnPlateau(opt2, 'min', verbose=True, patience=2)\n# for i in range(28):\n#     use_aug=True\n#     print(10+i)\n#     train_loss, train_acc = train_epoch(model, opt2, scheduler=cycle_scheduler, batchsize=32, use_aug=use_aug)\n#     clear_output()\n#     val_loss, val_acc = test(model, plato_scheduler=plato_scheduler)\n#     acc = torch.mean(torch.tensor(val_acc)).item()\n#     log_accs.append(acc)\n#     log_tr_accs.append(torch.mean(torch.tensor(train_acc[-(90000//32):])).item())\n# #     plt.plot(log_accs)\n# #     plt.plot(log_tr_accs)\n# #     plt.show()\n#     print(acc)\n","metadata":{"cellId":"ta6nz1kajpzjqxowqfcf8"},"outputs":[],"execution_count":1186},{"cell_type":"code","source":"#!g1.1\n# b_model = BestModel()\n\n# checkpoint = torch.load('model_best.pt')\n# b_model.load_state_dict(checkpoint['model_state_dict'])\n# # b_model2 = BestModel().load_state_dict(checkpoint['model_state_dict']).cuda()\n# b_model = b_model.cuda()","metadata":{"cellId":"k1lbd5m4rlnp7si3x2sg5"},"outputs":[],"execution_count":1346},{"cell_type":"code","source":"#!g1.1\ndef decr_rate(x):\n    if x % (90000//32 // 2) == 0:\n        print('decr')\n        return 0.5\n    return 1.0\n\n# model = b_model\n# opt1 = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n# opt3 = torch.optim.RMSprop(model.parameters(), lr=1e-3, weight_decay=1e-5)\n# opt2 = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-5)\n# cycle_scheduler = lr_scheduler.OneCycleLR(opt2, max_lr=1e-4, steps_per_epoch=90000//32+1, epochs=3)\n# step_scheduler = lr_scheduler.StepLR(opt3, step_size=90000//32//2, gamma=0.8)\n# cycle_scheduler2 = lr_scheduler.CyclicLR(opt2, base_lr=0.00001, max_lr=0.001, step_size_up=2*90000//32, cycle_momentum=False)\n# cos_scheduler = lr_scheduler.CosineAnnealingLR(opt2, T_max=3*90000//32)\n# plato_scheduler = lr_scheduler.ReduceLROnPlateau(opt2, 'min', verbose=True, patience=2)\n# exp_scheduler = lr_scheduler.MultiplicativeLR(opt2, decr_rate)\n# cos_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(opt3, T_0=90000//32, T_mult=2)\n\n\n\n# log_accs = []\n# log_tr_accs = []","metadata":{"cellId":"koob3zl3xxnf3e8wa55ds"},"outputs":[],"execution_count":1349},{"cell_type":"code","source":"#!g1.1\n# SEARCHING FOR THE BEST ARCHITECTURE\n\n# TURN_ON_DROPOUT = True\n\n# NUM_TO_FIND = 100\n# VAL_EPOCHS = 5\n# best_model = None\n# best_acc = 0\n# best_loss = 100\n# log_models = []\n# log_accs = []\n# for i in range(NUM_TO_FIND):\n#     num_downs = random.choice([3, 4])\n#     model = FindModel(min_depth=6, max_depth=12, start_num_channes=80, num_downs=num_downs, use_dropout=TURN_ON_DROPOUT).cuda()\n#     opt2 = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n#     loss, acc = train(model, opt2, VAL_EPOCHS, model_no=i)\n#     if loss < best_loss:\n#         best_model = model\n#         best_acc = acc.item()\n#         best_loss = loss\n#     log_models.append(model.represent())\n#     log_accs.append(acc)\n# print(best_acc)\n# print(best_model.represent())","metadata":{"cellId":"9b4acyj0s69ww76qwke8vb"},"outputs":[],"execution_count":1080},{"cell_type":"code","source":"#!g1.1\n# SEARCHING FOR BEST HYPERPARAMETRS\n\n# VAL_EPOCHS = 5\n# best_model = None\n# best_acc = 0\n# best_loss = 100\n\n# best_params = None\n\n# log_accs = []\n# log_params = []\n# eps = 1e-05\n# for momentum in [0.2, 0.5]:\n#     params = {'eps':eps, 'momentum':momentum, 'bias':False, 'p':0.3}\n#     model = BestModel(params=params).cuda()\n#     opt2 = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n#     loss, acc = train(model, opt2, VAL_EPOCHS, model_no=str(params))\n#     if loss < best_loss:\n#         best_model = model\n#         best_acc = acc.item()\n#         best_loss = loss\n#         best_params = params\n#     log_accs.append(acc.item())\n#     log_params.append(params)\n# print(best_acc)\n# print(best_params)","metadata":{"cellId":"0c0ol19h1cmg1oodmkv8o0x"},"outputs":[],"execution_count":1126},{"cell_type":"code","source":"#!g1.1\n#оцениваем на тесте\n\ndef ind_to_path_test(a):\n    s = str(a)\n    return 'simple_image_classification/test/test_' + '0' * (5 - len(s)) + s + '.jpg'\n\nmodel.eval()\n# b_model.eval()\ntrans = transforms.ToTensor()\nnorm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\npreds = []\nfor i in tqdm(range(10000)):\n    image = ind_to_path_test(i)\n    im=Image.open(image)\n    tens = trans(im)\n    tens = norm(tens)\n    tens = tens.reshape(1, *tens.shape)\n#     output = end_softmax(b_model(tens.cuda()))\n    output = end_softmax(model(tens.cuda()))\n    pred = torch.max(output, 1)[1]\n    preds.append(int(pred[0].item())) ","metadata":{"cellId":"az1fpextmlcgcwcmgd1t3","trusted":true},"outputs":[],"execution_count":1425},{"cell_type":"code","source":"#!g1.1\n#оформляем\nvf = np.vectorize(lambda x: 'test_' + '0' * (5 - len(str(x))) + str(x) + '.jpg')\ndf_ans = pd.DataFrame(vf(np.arange(10000)))\ndf_ans[1] = pd.Series(preds)\ndf_ans.columns = ['Id', 'Category']\ndf_ans['Category'] = df_ans.Category.apply(lambda x: '0' * (4 - len(str(x))) + str(x))\ndf_ans.set_index('Id').to_csv('labels_test.csv')","metadata":{"cellId":"1q7ku5sdtkjbimh6fxr97g","trusted":true},"outputs":[],"execution_count":1426},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"wcibx01qw3lxxd1o3u0k4r"},"outputs":[],"execution_count":null}]}